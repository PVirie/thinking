Goal pursuing with Hierarchical Uncertainty Minimization Across Networks.

Steer LLM by adjusting word embeddings.
Aka, control generation.
Inverse embedding to text by only modify the text embedder.
@inproceedings{han2024word,
  title={Word Embeddings Are Steers for Language Models},
  author={Han, Chi and Xu, Jialiang and Li, Manling and Fung, Yi and Sun, Chenkai and Jiang, Nan and Abdelzaher, Tarek and Ji, Heng},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={16410--16430},
  year={2024}
}
Inverse embedding by RL
@article{tennenholtz2024embedding,
  title={Embedding-Aligned Language Models},
  author={Tennenholtz, Guy and Chow, Yinlam and Hsu, Chih-Wei and Shani, Lior and Liang, Ethan and Boutilier, Craig},
  journal={arXiv preprint arXiv:2406.00024},
  year={2024}
}
Inverse embedding to text by training another model.
@article{li2023sentence,
  title={Sentence embedding leaks more information than you expect: Generative embedding inversion attack to recover the whole sentence},
  author={Li, Haoran and Xu, Mingshi and Song, Yangqiu},
  journal={arXiv preprint arXiv:2305.03010},
  year={2023}
}
@article{morris2023text,
  title={Text embeddings reveal (almost) as much as text},
  author={Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly and Rush, Alexander M},
  journal={arXiv preprint arXiv:2310.06816},
  year={2023}
}

Speed up LLM inference
Can apply techniques to evaluate chain of thoughts in parallel.
@inproceedings{miao2024specinfer,
  title={Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={932--949},
  year={2024}
}

LLM
Tree of thoughts, use LLM to generate trees instead of next step, then use A* to search the tree, back track when necessary.

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}


Hierarchy RL

Using hierarchy and Decoder only transformer to solve RL problem. 
The hierarchy is learned from the environment end-to-end.
@article{huang2024context,
  title={In-Context Decision Transformer: Reinforcement Learning via Hierarchical Chain-of-Thought},
  author={Huang, Sili and Hu, Jifeng and Chen, Hechang and Sun, Lichao and Yang, Bo},
  journal={arXiv preprint arXiv:2405.20692},
  year={2024}
}

@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}

Multigoal model based RL with graph for planning.

@inproceedings{zhang2021world,
  title={World model as a graph: Learning latent landmarks for planning},
  author={Zhang, Lunjun and Yang, Ge and Stadie, Bradly C},
  booktitle={International Conference on Machine Learning},
  pages={12611--12620},
  year={2021},
  organization={PMLR}
}

Quantum reinforcement learning

@article{dong2008quantum,
  title={Quantum reinforcement learning},
  author={Dong, Daoyi and Chen, Chunlin and Li, Hanxiong and Tarn, Tzyh-Jong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={38},
  number={5},
  pages={1207--1220},
  year={2008},
  publisher={IEEE}
}

Local matching yields good representation.

@article{goldberg2014word2vec,
  title={word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
  author={Goldberg, Yoav and Levy, Omer},
  journal={arXiv preprint arXiv:1402.3722},
  year={2014}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{lowe2019putting,
  title={Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
  author={L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3033--3045},
  year={2019}
}


Tripplet loss without negative sampling for metric learning.
@inproceedings{qian2019softtriple,
  title={Softtriple loss: Deep metric learning without triplet sampling},
  author={Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6450--6458},
  year={2019}
}



Graph based embedding

The goal is to build a nice space such that we always have a clue to go from any point s to to any point d in the cognitive map.
The cognitive map always maintains straightforward direction to goals.

UMAP!!!
UMAP does not solve my vice. It still produces non-convex map. If we want to go from s to t, we may stuck at a local optima.
(Check the image of the elephant's embedding here https://pair-code.github.io/understanding-umap/ to see UMAP results.)

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}



Learning to compare with matching
Best results for continual learning is learning to compare...

@inproceedings{sung2018learning,
  title={Learning to compare: Relation network for few-shot learning},
  author={Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1199--1208},
  year={2018}
}

Cost algebra: A* can be used on any search structure with cost that satisfies the following algebraic rules.

@inproceedings{edelkamp2005cost,
  title={Cost-algebraic heuristic search},
  author={Edelkamp, Stefan and Jabbar, Shahid and Lluch-Lafuente, Alberto},
  booktitle={AAAI},
  pages={1362--1367},
  year={2005}
}


To help define heuristics for search, one could use embedding. But embedding compromises network structure. (Only in low dimensional space, embedding can perfectly capture all the variance when the number of dimensions equal the number of nodes.) We find a way without the need to compromise, that's to use the classic neighbor table. Embedding is good because memory space is O(V) while our table is O(V*n) where n is the size of neighbor. But the optimal planner requires we maintain the n = V. We use hierarchy to help planning, comprimise the result but saves space and searching time instead.

@article{goyal2018graph,
  title={Graph embedding techniques, applications, and performance: A survey},
  author={Goyal, Palash and Ferrara, Emilio},
  journal={Knowledge-Based Systems},
  volume={151},
  pages={78--94},
  year={2018},
  publisher={Elsevier}
}

Energy functions, recent work:

@article{mordatch2018concept,
  title={Concept learning with energy-based models},
  author={Mordatch, Igor},
  journal={arXiv preprint arXiv:1811.02486},
  year={2018}
}


Using graph and A* to plan robot. Solving high-dimensional sensory input with graph.
Selecting node with merging to promote sparsity.
@article{emmons2020sparse,
  title={Sparse Graphical Memory for Robust Planning},
  author={Emmons, Scott and Jain, Ajay and Laskin, Misha and Kurutach, Thanard and Abbeel, Pieter and Pathak, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

Using Language model with Skill to find the best action.
@article{ahn2022can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Ahn, Michael and Brohan, Anthony and Brown, Noah and Chebotar, Yevgen and Cortes, Omar and David, Byron and Finn, Chelsea and Gopalakrishnan, Keerthana and Hausman, Karol and Herzog, Alex and others},
  journal={arXiv preprint arXiv:2204.01691},
  year={2022}
}


Turing completeness of modern deep learning architecture
@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}


Learning embedding space with Brownian bridge
@article{wang2022language,
  title={Language modeling via stochastic processes},
  author={Wang, Rose E and Durmus, Esin and Goodman, Noah and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2203.11370},
  year={2022}
}


Q* algorithm for LLMs
@article{wang2024q,
  title={Q*: Improving Multi-step Reasoning for LLMs with Deliberative Planning},
  author={Wang, Chaojie and Deng, Yanchen and Lv, Zhiyi and Yan, Shuicheng and Bo, An},
  journal={arXiv preprint arXiv:2406.14283},
  year={2024}
}