Goal pursuing with Hierarchical Uncertainty Minimization Across Networks.

Hierarchy RL


@article{nachum2018near,
  title={Near-optimal representation learning for hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.01257},
  year={2018}
}


Quantum reinforcement learning

@article{dong2008quantum,
  title={Quantum reinforcement learning},
  author={Dong, Daoyi and Chen, Chunlin and Li, Hanxiong and Tarn, Tzyh-Jong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
  volume={38},
  number={5},
  pages={1207--1220},
  year={2008},
  publisher={IEEE}
}

Local matching yields good representation.

@article{goldberg2014word2vec,
  title={word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method},
  author={Goldberg, Yoav and Levy, Omer},
  journal={arXiv preprint arXiv:1402.3722},
  year={2014}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{lowe2019putting,
  title={Putting An End to End-to-End: Gradient-Isolated Learning of Representations},
  author={L{\"o}we, Sindy and O'Connor, Peter and Veeling, Bastiaan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3033--3045},
  year={2019}
}

Graph based embedding

The goal is to build a nice space such that we always have a clue to go from any point s to to any point d in the cognitive map.
The cognitive map always maintains straightforward direction to goals.

UMAP!!!
UMAP does not solve my vice. It still produces non-convex map. If we want to go from s to t, we may stuck at a local optima.
(Check the image of the elephant's embedding here https://pair-code.github.io/understanding-umap/ to see UMAP results.)

@article{mcinnes2018umap,
  title={Umap: Uniform manifold approximation and projection for dimension reduction},
  author={McInnes, Leland and Healy, John and Melville, James},
  journal={arXiv preprint arXiv:1802.03426},
  year={2018}
}



Learning to compare with matching
Best results for continual learning is learning to compare...

@inproceedings{sung2018learning,
  title={Learning to compare: Relation network for few-shot learning},
  author={Sung, Flood and Yang, Yongxin and Zhang, Li and Xiang, Tao and Torr, Philip HS and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1199--1208},
  year={2018}
}

Cost algebra: A* can be used on any search structure with cost that satisfies the following algebraic rules.

@inproceedings{edelkamp2005cost,
  title={Cost-algebraic heuristic search},
  author={Edelkamp, Stefan and Jabbar, Shahid and Lluch-Lafuente, Alberto},
  booktitle={AAAI},
  pages={1362--1367},
  year={2005}
}


To help define heuristics for search, one could use embedding. But embedding compromises network structure. (Only in low dimensional space, embedding can perfectly capture all the variance when the number of dimensions equal the number of nodes.) We find a way without the need to compromise, that's to use the classic neighbor table. Embedding is good because memory space is O(V) while our table is O(V*n) where n is the size of neighbor. But the optimal planner requires we maintain the n = V. We use hierarchy to help planning, comprimise the result but saves space and searching time instead.

@article{goyal2018graph,
  title={Graph embedding techniques, applications, and performance: A survey},
  author={Goyal, Palash and Ferrara, Emilio},
  journal={Knowledge-Based Systems},
  volume={151},
  pages={78--94},
  year={2018},
  publisher={Elsevier}
}

Energy functions, recent work:

@article{mordatch2018concept,
  title={Concept learning with energy-based models},
  author={Mordatch, Igor},
  journal={arXiv preprint arXiv:1811.02486},
  year={2018}
}


Using graph and A* to plan robot. Solving high-dimensional sensory input with graph.
Selecting node with merging to promote sparsity.
@article{emmons2020sparse,
  title={Sparse Graphical Memory for Robust Planning},
  author={Emmons, Scott and Jain, Ajay and Laskin, Misha and Kurutach, Thanard and Abbeel, Pieter and Pathak, Deepak},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}


Turing completeness of modern deep learning architecture
@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}